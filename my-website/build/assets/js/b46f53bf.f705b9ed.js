"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4501],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var r=i(6540);const t={},s=r.createContext(t);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(s.Provider,{value:n},e.children)}},9055:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-4-vla/whisper","title":"Whisper Integration","description":"Add voice recognition with OpenAI Whisper","source":"@site/docs/module-4-vla/whisper.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/whisper","permalink":"/e-book-hackathon-2025/docs/module-4-vla/whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/faizanmustafa/e-book-hackathon-2025/tree/main/my-website/docs/module-4-vla/whisper.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Whisper Integration","description":"Add voice recognition with OpenAI Whisper"},"sidebar":"learningSidebar","previous":{"title":"Module 4: VLA & AI Brain","permalink":"/e-book-hackathon-2025/docs/module-4-vla/intro"},"next":{"title":"LLM Task Planner","permalink":"/e-book-hackathon-2025/docs/module-4-vla/llm-planner"}}');var t=i(4848),s=i(8453);const o={sidebar_position:2,title:"Whisper Integration",description:"Add voice recognition with OpenAI Whisper"},l="Whisper Integration",a={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Installation",id:"installation",level:2},{value:"Voice Capture Node",id:"voice-capture-node",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"whisper-integration",children:"Whisper Integration"})}),"\n",(0,t.jsx)(n.p,{children:"Integrate OpenAI Whisper for real-time voice recognition."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Install Whisper locally"}),"\n",(0,t.jsx)(n.li,{children:"Create a voice capture node"}),"\n",(0,t.jsx)(n.li,{children:"Process audio in real-time"}),"\n",(0,t.jsx)(n.li,{children:"Publish transcriptions to ROS 2"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\npip install sounddevice numpy\n"})}),"\n",(0,t.jsx)(n.h2,{id:"voice-capture-node",children:"Voice Capture Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="whisper_node.py"',children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport sounddevice as sd\nimport numpy as np\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__('whisper_node')\n        self.publisher = self.create_publisher(String, '/voice/transcription', 10)\n        self.model = whisper.load_model(\"base\")\n        self.get_logger().info('Whisper node ready')\n\n    def transcribe(self, audio):\n        result = self.model.transcribe(audio)\n        msg = String()\n        msg.data = result[\"text\"]\n        self.publisher.publish(msg)\n        return result[\"text\"]\n"})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./llm-planner",children:"Continue to LLM Planner \u2192"})})})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);