"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[2101],{6404:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>a,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/intro","title":"Module 4: VLA & AI Brain","description":"Integrate Vision-Language-Action models to give your robot intelligence","source":"@site/docs/module-4-vla/intro.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/intro","permalink":"/e-book-hackathon-2025/docs/module-4-vla/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/faizanmustafa/e-book-hackathon-2025/tree/main/my-website/docs/module-4-vla/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Module 4: VLA & AI Brain","description":"Integrate Vision-Language-Action models to give your robot intelligence"},"sidebar":"learningSidebar","previous":{"title":"Troubleshooting","permalink":"/e-book-hackathon-2025/docs/module-3-isaac/troubleshooting"},"next":{"title":"Whisper Integration","permalink":"/e-book-hackathon-2025/docs/module-4-vla/whisper"}}');var r=i(4848),l=i(8453);const s={sidebar_position:1,title:"Module 4: VLA & AI Brain",description:"Integrate Vision-Language-Action models to give your robot intelligence"},d="Module 4: VLA & AI Brain",o={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"The VLA Pipeline",id:"the-vla-pipeline",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"What You&#39;ll Build",id:"what-youll-build",level:2},{value:"VLA Architecture",id:"vla-architecture",level:2},{value:"Latency Budget",id:"latency-budget",level:2},{value:"Technology Options",id:"technology-options",level:2},{value:"Speech-to-Text",id:"speech-to-text",level:3},{value:"LLM Planning",id:"llm-planning",level:3},{value:"Action Schema Preview",id:"action-schema-preview",level:2},{value:"Ready to Begin?",id:"ready-to-begin",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"module-4-vla--ai-brain",children:"Module 4: VLA & AI Brain"})}),"\n",(0,r.jsx)(n.p,{children:"Welcome to Module 4! This is where your robot gains intelligence. You'll integrate Vision-Language-Action (VLA) models to create a system that understands voice commands and translates them into robot actions."}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Integrate Whisper for real-time voice recognition"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Connect LLMs (Claude/Llama) for task planning"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Design action schemas for robot commands"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Build a complete VLA pipeline"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Handle errors and recovery gracefully"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Optimize latency for responsive interaction"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"the-vla-pipeline",children:"The VLA Pipeline"}),"\n",(0,r.jsx)(n.mermaid,{value:"flowchart LR\n    A[\ud83c\udfa4 Voice] --\x3e B[Whisper]\n    B --\x3e C[Text]\n    C --\x3e D[\ud83e\udde0 LLM]\n    D --\x3e E[Action Plan]\n    E --\x3e F[ROS 2]\n    F --\x3e G[\ud83e\udd16 Robot]\n\n    style A fill:#6c757d,color:#fff\n    style B fill:#1e3a5f,color:#fff\n    style D fill:#e65c00,color:#fff\n    style F fill:#1e3a5f,color:#fff\n    style G fill:#28a745,color:#fff"}),"\n",(0,r.jsx)(n.p,{children:"This is the core of Physical AI - transforming human intent into robot behavior:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Stage"}),(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Function"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Input"})}),(0,r.jsx)(n.td,{children:"Microphone"}),(0,r.jsx)(n.td,{children:"Capture voice command"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Speech-to-Text"})}),(0,r.jsx)(n.td,{children:"Whisper"}),(0,r.jsx)(n.td,{children:"Convert audio to text"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Understanding"})}),(0,r.jsx)(n.td,{children:"LLM"}),(0,r.jsx)(n.td,{children:"Parse intent, plan actions"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Execution"})}),(0,r.jsx)(n.td,{children:"ROS 2"}),(0,r.jsx)(n.td,{children:"Dispatch robot commands"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Feedback"})}),(0,r.jsx)(n.td,{children:"Sensors"}),(0,r.jsx)(n.td,{children:"Monitor task progress"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Chapter"}),(0,r.jsx)(n.th,{children:"Topic"}),(0,r.jsx)(n.th,{children:"Duration"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"./whisper",children:"Whisper Integration"})}),(0,r.jsx)(n.td,{children:"90 min"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"2"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"./llm-planner",children:"LLM Task Planner"})}),(0,r.jsx)(n.td,{children:"120 min"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"3"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"./vla-pipeline",children:"VLA Pipeline"})}),(0,r.jsx)(n.td,{children:"120 min"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"./troubleshooting",children:"Troubleshooting"})}),(0,r.jsx)(n.td,{children:"Reference"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Total Time: 8-10 hours"})}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before starting this module, ensure you have:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Completed Modules 1-3"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Working ROS 2 and Gazebo setup"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Python environment with pip"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 API key for Claude or OpenAI (or local Llama setup)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Microphone for voice input (optional for testing)"}),"\n"]}),"\n",(0,r.jsxs)(n.admonition,{title:"API Keys",type:"tip",children:[(0,r.jsx)(n.p,{children:"You'll need either:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Anthropic API key"})," for Claude (recommended)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI API key"})," for GPT-4"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Local setup"})," for Llama 3.1 (requires RTX GPU)"]}),"\n"]})]}),"\n",(0,r.jsx)(n.h2,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,r.jsx)(n.p,{children:"A complete voice-to-action system:"}),"\n",(0,r.jsx)(n.mermaid,{value:'sequenceDiagram\n    participant User\n    participant Whisper\n    participant LLM\n    participant ROS2\n    participant Robot\n\n    User->>Whisper: "Pick up the red cup"\n    Whisper->>LLM: transcription: "Pick up the red cup"\n\n    Note over LLM: Parse intent<br/>Identify object<br/>Plan actions\n\n    LLM->>ROS2: ActionPlan {<br/>  navigate_to: table,<br/>  detect: red_cup,<br/>  grasp: red_cup<br/>}\n\n    ROS2->>Robot: /navigate_to_goal\n    Robot->>ROS2: goal_reached\n    ROS2->>Robot: /detect_objects\n    Robot->>ROS2: red_cup @ (x,y,z)\n    ROS2->>Robot: /grasp_object\n    Robot->>ROS2: grasp_complete\n\n    ROS2->>LLM: task_complete\n    LLM->>User: "I\'ve picked up the red cup"'}),"\n",(0,r.jsx)(n.h2,{id:"vla-architecture",children:"VLA Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph Input["Input Processing"]\n        A[Microphone] --\x3e B[Audio Buffer]\n        B --\x3e C[Whisper STT]\n    end\n\n    subgraph Brain["AI Brain"]\n        C --\x3e D[Intent Parser]\n        D --\x3e E[Task Planner]\n        E --\x3e F[Action Generator]\n    end\n\n    subgraph Execution["ROS 2 Execution"]\n        F --\x3e G[Action Client]\n        G --\x3e H[Navigation]\n        G --\x3e I[Manipulation]\n        G --\x3e J[Perception]\n    end\n\n    subgraph Feedback["Feedback Loop"]\n        H --\x3e K[Status Monitor]\n        I --\x3e K\n        J --\x3e K\n        K --\x3e E\n    end\n\n    style C fill:#1e3a5f,color:#fff\n    style E fill:#e65c00,color:#fff\n    style G fill:#1e3a5f,color:#fff'}),"\n",(0,r.jsx)(n.h2,{id:"latency-budget",children:"Latency Budget"}),"\n",(0,r.jsx)(n.p,{children:"For responsive interaction, the total pipeline must complete within 3 seconds:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Stage"}),(0,r.jsx)(n.th,{children:"Target"}),(0,r.jsx)(n.th,{children:"Actual"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Audio capture"}),(0,r.jsx)(n.td,{children:"100ms"}),(0,r.jsx)(n.td,{children:"-"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Whisper STT"}),(0,r.jsx)(n.td,{children:"500ms"}),(0,r.jsx)(n.td,{children:"-"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLM planning"}),(0,r.jsx)(n.td,{children:"1500ms"}),(0,r.jsx)(n.td,{children:"-"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ROS 2 dispatch"}),(0,r.jsx)(n.td,{children:"100ms"}),(0,r.jsx)(n.td,{children:"-"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Total"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"<3000ms"})}),(0,r.jsx)(n.td,{children:"-"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"You'll measure and optimize these during the module."}),"\n",(0,r.jsx)(n.h2,{id:"technology-options",children:"Technology Options"}),"\n",(0,r.jsx)(n.h3,{id:"speech-to-text",children:"Speech-to-Text"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Option"}),(0,r.jsx)(n.th,{children:"Latency"}),(0,r.jsx)(n.th,{children:"Privacy"}),(0,r.jsx)(n.th,{children:"Quality"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Whisper (local)"}),(0,r.jsx)(n.td,{children:"~500ms"}),(0,r.jsx)(n.td,{children:"\u2705 High"}),(0,r.jsx)(n.td,{children:"Excellent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Whisper API"}),(0,r.jsx)(n.td,{children:"~300ms"}),(0,r.jsx)(n.td,{children:"\u274c Cloud"}),(0,r.jsx)(n.td,{children:"Excellent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Google STT"}),(0,r.jsx)(n.td,{children:"~200ms"}),(0,r.jsx)(n.td,{children:"\u274c Cloud"}),(0,r.jsx)(n.td,{children:"Good"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"llm-planning",children:"LLM Planning"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Option"}),(0,r.jsx)(n.th,{children:"Latency"}),(0,r.jsx)(n.th,{children:"Cost"}),(0,r.jsx)(n.th,{children:"Quality"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Claude 3.5 Sonnet"}),(0,r.jsx)(n.td,{children:"~1s"}),(0,r.jsx)(n.td,{children:"$$"}),(0,r.jsx)(n.td,{children:"Excellent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GPT-4"}),(0,r.jsx)(n.td,{children:"~1.5s"}),(0,r.jsx)(n.td,{children:"$$$"}),(0,r.jsx)(n.td,{children:"Excellent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Llama 3.1 (local)"}),(0,r.jsx)(n.td,{children:"~2s"}),(0,r.jsx)(n.td,{children:"Free"}),(0,r.jsx)(n.td,{children:"Good"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"action-schema-preview",children:"Action Schema Preview"}),"\n",(0,r.jsx)(n.p,{children:"You'll define structured action schemas like this:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",metastring:'title="action_schema.json"',children:'{\n  "action": "pick_object",\n  "parameters": {\n    "object_id": "red_cup",\n    "location": {"x": 1.2, "y": 0.5, "z": 0.8},\n    "grasp_type": "power"\n  },\n  "preconditions": [\n    "robot_at_table",\n    "object_detected"\n  ],\n  "effects": [\n    "holding_object"\n  ]\n}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"ready-to-begin",children:"Ready to Begin?"}),"\n",(0,r.jsx)(n.p,{children:"Let's give your robot a brain:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"./whisper",children:"Start Chapter 1: Whisper Integration \u2192"})})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Module Progress:"})," 0/4 chapters completed"]})]})}function a(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>d});var t=i(6540);const r={},l=t.createContext(r);function s(e){const n=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(l.Provider,{value:n},e.children)}}}]);